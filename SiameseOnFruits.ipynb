{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these folders has 10 different images of a single person taken from various angles. For an instance, let us open folder s1. As you can see, there are 10 different images of a single person:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will open and check folder s13,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that siamese networks require inputs as a pair along with the label, we have to create our data in such a way. So we will take two images randomly from the same folder and mark it as a genuine pair and we will take a single image from two different folders and mark them as an imposite pair. A sample data is shown in the below figure, as you can notice a genuine pair has images of the same person and imposite pair has images of a different person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data as pairs along with their labels, we train our siamese network. From the image pair, we feed one image to the network A and another image to the network B. The role of these two networks is only to extract the feature vectors. So, we use two convolution layers with relu activations for extracting the features. Once we have learned the feature, we feed the resultant feature vector from both of the networks to the energy function which measures the similarity,  we use Euclidean distance as our energy function. So, we train our network by feeding the image pair to learn the semantic similarity between them.  Now, we will see this step by step. \n",
    "\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = \"fruits_data_set_2017_12_27\\\\Training\\\\f\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function for reading our input image. The function read_image takes input as an image and returns the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename, byteorder='>'):\n",
    "    \n",
    "    #first we read the image, as a raw file to the buffer\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    \n",
    "    #using regex, we extract the header, width, height and maxval of the image\n",
    "    header, width, height, maxval = re.search(\n",
    "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
    "    \n",
    "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100 at 0x216691E4748>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(rootPath + \"1\\\\0_100.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed this image to our read_image function, it will return as the numpy array,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5f49029a8a58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrootPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"1\\\\0_100.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-9b7eeb7a3833>\u001b[0m in \u001b[0;36mread_image\u001b[1;34m(filename, byteorder)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;34mb\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;34mb\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groups'"
     ]
    }
   ],
   "source": [
    "img = read_image(rootPath + \"1\\\\0_100.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of converting an image with the Keras API\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(rootPath + \"1\\\\0_100.jpg\", color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "i = 0\n",
    "dir1 = rootPath + str(i+1)\n",
    "images = os.listdir(rootPath + str(i+1))\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2\n",
    "total_sample_size = 10000\n",
    "\n",
    "\n",
    "def get_data2(size, total_sample_size):\n",
    "    #read the image\n",
    "    image_p = load_img(rootPath + \"1\\\\0_100.jpg\")\n",
    "    image = img_to_array(image_p)\n",
    "    #reduce the size\n",
    "    image = image[::size, ::size]\n",
    "    #get the new size\n",
    "    dim1 = image.shape[0]\n",
    "    dim2 = image.shape[1]\n",
    "    channels = image.shape[2]\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, dim1, dim2, channels])  # 2 is for pairs\n",
    "    y_genuine = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(5):\n",
    "        for j in range(int(total_sample_size/5)):\n",
    "            ind1 = 0\n",
    "            ind2 = 0\n",
    "            \n",
    "            dir1 = rootPath + str(i+1)\n",
    "            images = os.listdir(rootPath + str(i+1))\n",
    "            no_images = len(images)\n",
    "            \n",
    "            #read images from same directory (genuine pair)\n",
    "            while ind1 == ind2:\n",
    "                ind1 = np.random.randint(no_images - 1)\n",
    "                ind2 = np.random.randint(no_images - 1)\n",
    "            \n",
    "            # read the two images\n",
    "            #img1 = read_image('att_faces\\\\s' + str(i+1) + '\\\\' + str(ind1 + 1) + '.pgm', 'rw+')\n",
    "            #img2 = read_image('att_faces\\\\s' + str(i+1) + '\\\\' + str(ind2 + 1) + '.pgm', 'rw+')\n",
    "            #print(images[ind1])\n",
    "            image1 = load_img(dir1 + \"\\\\\" + images[ind1])\n",
    "            image2 = load_img(dir1 + \"\\\\\" + images[ind2])\n",
    "            \n",
    "            img1 = img_to_array(image1)\n",
    "            img2 = img_to_array(image2)\n",
    "            \n",
    "            #reduce the size\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "            \n",
    "            #store the images to the initialized numpy array\n",
    "            x_geuine_pair[count, 0, :, :, :] = img1\n",
    "            x_geuine_pair[count, 1, :, :, :] = img2\n",
    "            \n",
    "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
    "            y_genuine[count] = 1\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, dim1, dim2, channels])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(int(total_sample_size/400)):\n",
    "        for j in range(400):\n",
    "            \n",
    "            #read images from different directory (imposite pair)\n",
    "            while True:\n",
    "                ind1 = np.random.randint(5)\n",
    "                ind2 = np.random.randint(5)\n",
    "                if ind1 != ind2:\n",
    "                    break\n",
    "                    \n",
    "            dir1 = rootPath + str(ind1+1)\n",
    "            dir2 = rootPath + str(ind2+1)\n",
    "            images1 = os.listdir(dir1)\n",
    "            images2 = os.listdir(dir2)\n",
    "            #img1 = read_image('att_faces\\\\s' + str(ind1+1) + '\\\\' + str(j + 1) + '.pgm', 'rw+')\n",
    "            #img2 = read_image('att_faces\\\\s' + str(ind2+1) + '\\\\' + str(j + 1) + '.pgm', 'rw+')\n",
    "\n",
    "            image1 = load_img(dir1 + \"\\\\\" + images1[j+1])\n",
    "            image2 = load_img(dir2 + \"\\\\\" + images2[j+1])\n",
    "            \n",
    "            img1 = img_to_array(image1)\n",
    "            img2 = img_to_array(image2)\n",
    "            \n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "\n",
    "            x_imposite_pair[count, 0, :, :, :] = img1\n",
    "            x_imposite_pair[count, 1, :, :, :] = img2\n",
    "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
    "            y_imposite[count] = 0\n",
    "            count += 1\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2\n",
    "total_sample_size = 10000\n",
    "\n",
    "\n",
    "def get_data(size, total_sample_size):\n",
    "    #read the image\n",
    "    image_p = load_img(rootPath + \"1\\\\0_100.jpg\", color_mode=\"grayscale\")\n",
    "    image = img_to_array(image_p)\n",
    "    #reduce the size\n",
    "    image = image[::size, ::size]\n",
    "    #get the new size\n",
    "    dim1 = image.shape[0]\n",
    "    dim2 = image.shape[1]\n",
    "    dim3 = image.shape[2]\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, dim1, dim2, 1])  # 2 is for pairs\n",
    "    y_genuine = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(5):\n",
    "        for j in range(int(total_sample_size/5)):\n",
    "            ind1 = 0\n",
    "            ind2 = 0\n",
    "            \n",
    "            dir1 = rootPath + str(i+1)\n",
    "            images = os.listdir(rootPath + str(i+1))\n",
    "            no_images = len(images)\n",
    "            \n",
    "            #read images from same directory (genuine pair)\n",
    "            while ind1 == ind2:\n",
    "                ind1 = np.random.randint(no_images - 1)\n",
    "                ind2 = np.random.randint(no_images - 1)\n",
    "            \n",
    "            # read the two images\n",
    "            #img1 = read_image('att_faces\\\\s' + str(i+1) + '\\\\' + str(ind1 + 1) + '.pgm', 'rw+')\n",
    "            #img2 = read_image('att_faces\\\\s' + str(i+1) + '\\\\' + str(ind2 + 1) + '.pgm', 'rw+')\n",
    "            #print(images[ind1])\n",
    "            image1 = load_img(dir1 + \"\\\\\" + images[ind1], color_mode = \"grayscale\")\n",
    "            image2 = load_img(dir1 + \"\\\\\" + images[ind2], color_mode = \"grayscale\")\n",
    "            \n",
    "            img1 = img_to_array(image1)\n",
    "            img2 = img_to_array(image2)\n",
    "            \n",
    "            #reduce the size\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "            \n",
    "            #store the images to the initialized numpy array\n",
    "            x_geuine_pair[count, 0, :, :, :] = img1\n",
    "            x_geuine_pair[count, 1, :, :, :] = img2\n",
    "            \n",
    "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
    "            y_genuine[count] = 1\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, dim1, dim2, 1])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(int(total_sample_size/400)):\n",
    "        for j in range(400):\n",
    "            \n",
    "            #read images from different directory (imposite pair)\n",
    "            while True:\n",
    "                ind1 = np.random.randint(5)\n",
    "                ind2 = np.random.randint(5)\n",
    "                if ind1 != ind2:\n",
    "                    break\n",
    "                    \n",
    "            dir1 = rootPath + str(ind1+1)\n",
    "            dir2 = rootPath + str(ind2+1)\n",
    "            images1 = os.listdir(dir1)\n",
    "            images2 = os.listdir(dir2)\n",
    "            #img1 = read_image('att_faces\\\\s' + str(ind1+1) + '\\\\' + str(j + 1) + '.pgm', 'rw+')\n",
    "            #img2 = read_image('att_faces\\\\s' + str(ind2+1) + '\\\\' + str(j + 1) + '.pgm', 'rw+')\n",
    "\n",
    "            image1 = load_img(dir1 + \"\\\\\" + images1[j+1], color_mode = \"grayscale\")\n",
    "            image2 = load_img(dir2 + \"\\\\\" + images2[j+1], color_mode = \"grayscale\")\n",
    "            \n",
    "            img1 = img_to_array(image1)\n",
    "            img2 = img_to_array(image2)\n",
    "            \n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "\n",
    "            x_imposite_pair[count, 0, :, :, :] = img1\n",
    "            x_imposite_pair[count, 1, :, :, :] = img2\n",
    "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
    "            y_imposite[count] = 0\n",
    "            count += 1\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data(size, total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2, 50, 50, 1)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=50x50 at 0x21604326B00>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "\n",
    "loadedImage = X[0,0,:,:,:]\n",
    "index = np.random.randint(5000)\n",
    "save_img('sampleData' + str(index) + '.jpg', loadedImage)\n",
    "# load the image to confirm it was saved correctly\n",
    "Image.open('sampleData' + str(index) + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    nb_filter = [6, 12]\n",
    "    kernel_size = 3\n",
    "    \n",
    "    \n",
    "    #convolutional layer 1\n",
    "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
    "                          border_mode='valid', dim_ordering='tf'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    seq.add(Dropout(.25))\n",
    "    \n",
    "    #convolutional layer 2\n",
    "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='tf'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf')) \n",
    "    seq.add(Dropout(.25))\n",
    "\n",
    "    #flatten \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(50, activation='relu'))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "def initialize_weights(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "\n",
    "def initialize_bias(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "\n",
    "def build_base_network2(input_shape):\n",
    "    \n",
    "    model = Sequential()   \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_37:0' shape=(?, 50, 50, 1) dtype=float32>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = x_train.shape[2:]\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)\n",
    "img_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we feed the image pair, to the base network, which will return the embeddings that is, feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=50x50 at 0x2160433D438>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "loadedImage = x_train[1,0,:,:,:]\n",
    "index = np.random.randint(5000)\n",
    "save_img('sampleData' + str(index) + '.jpg', loadedImage)\n",
    "# load the image to confirm it was saved correctly\n",
    "Image.open('sampleData' + str(index) + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himasing\\.conda\\envs\\tf-cpu\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), data_format=\"channels_last\", padding=\"valid\", input_shape=(50, 50, 1...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\himasing\\.conda\\envs\\tf-cpu\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), data_format=\"channels_last\", padding=\"valid\")`\n",
      "C:\\Users\\himasing\\.conda\\envs\\tf-cpu\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")`\n"
     ]
    }
   ],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(img_a)\n",
    "feat_vecs_b = base_network(img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequential_22/dense_12/Relu:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_vecs_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we set the epoch length to 13 and we use RMS prop for optimization and define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himasing\\.conda\\envs\\tf-cpu\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model = Model(input=[img_a, img_b], output=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function as contrastive_loss function and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = x_train[:, 0]\n",
    "img2 = x_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himasing\\.conda\\envs\\tf-cpu\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11250 samples, validate on 3750 samples\n",
      "Epoch 1/10\n",
      " - 30s - loss: 0.1097 - val_loss: 0.0412\n",
      "Epoch 2/10\n",
      " - 28s - loss: 0.0446 - val_loss: 0.0241\n",
      "Epoch 3/10\n",
      " - 27s - loss: 0.0254 - val_loss: 0.0123\n",
      "Epoch 4/10\n",
      " - 27s - loss: 0.0161 - val_loss: 0.0102\n",
      "Epoch 5/10\n",
      " - 27s - loss: 0.0112 - val_loss: 0.0075\n",
      "Epoch 6/10\n",
      " - 27s - loss: 0.0086 - val_loss: 0.0036\n",
      "Epoch 7/10\n",
      " - 27s - loss: 0.0061 - val_loss: 0.0047\n",
      "Epoch 8/10\n",
      " - 27s - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 9/10\n",
      " - 27s - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 10/10\n",
      " - 31s - loss: 0.0029 - val_loss: 9.4019e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216045fc198>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([img_1, img2], y_train, validation_split=.25,\n",
    "          batch_size=128, verbose=2, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check our model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04740208],\n",
       "       [ 0.95379859],\n",
       "       [ 1.24032927],\n",
       "       ..., \n",
       "       [ 0.04375583],\n",
       "       [ 0.02539294],\n",
       "       [ 1.3829031 ]], dtype=float32)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
